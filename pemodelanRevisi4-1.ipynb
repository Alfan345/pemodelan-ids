{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {},
      "source": [
        "# Modeling Multiclass IDS (Collapsed 9 Classes) — v4\n",
        "\n",
        "Fokus:\n",
        "- DNN reguler (feedforward), ReLU + Dropout (tanpa residual)\n",
        "- Class weight, AMP, early stopping, gradient clipping\n",
        "- Transform pipeline locked (fit sekali, reuse)\n",
        "- Phase A: batch eksplorasi progresif (tanpa adaptive)\n",
        "- Phase B: final training (default tanpa adaptive; opsi banding adaptive)\n",
        "- Simpan artefak (state_dict, scaler, cols, heavy_cols, config, report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "imports",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full data shape: (2568389, 83)\n",
            "Phase A shape: (571603, 83)\n",
            "Kelas: 5\n",
            "Fitur final: 58\n"
          ]
        }
      ],
      "source": [
        "# !pip install pandas numpy scikit-learn joblib torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json, joblib\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "ART_DIR = Path(\"./artifacts_multiclass_collapsed_v4\")\n",
        "data_art = joblib.load(ART_DIR / 'lite_clean_data_collapsed.pkl')\n",
        "full_df = data_art['full_df']\n",
        "phaseA_df = data_art['phaseA_df']\n",
        "label_map = data_art['label_map']\n",
        "inv_label_map = data_art['inv_label_map']\n",
        "LABEL_COL = data_art['label_col']\n",
        "feature_cols = data_art['feature_cols']\n",
        "num_classes = len(label_map)\n",
        "\n",
        "print(\"Full data shape:\", full_df.shape)\n",
        "print(\"Phase A shape:\", phaseA_df.shape)\n",
        "print(\"Kelas:\", num_classes)\n",
        "print(\"Fitur final:\", len(feature_cols))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "transform",
      "metadata": {},
      "source": [
        "## 1. Transform Pipeline (fit & apply — locked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "transform-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "SKEW_THRESHOLD = 1.5\n",
        "USE_ROBUST_SCALER = True\n",
        "\n",
        "def transform_fit(X_train_df, X_test_df, skew_threshold=1.5, use_robust=True):\n",
        "    X_train_df = X_train_df.apply(pd.to_numeric, errors='coerce')\n",
        "    X_test_df  = X_test_df.apply(pd.to_numeric, errors='coerce')\n",
        "    # Imputasi median berdasar train\n",
        "    medians = {}\n",
        "    for c in X_train_df.columns:\n",
        "        if X_train_df[c].isna().any():\n",
        "            med = X_train_df[c].median()\n",
        "            medians[c] = med\n",
        "            X_train_df[c] = X_train_df[c].fillna(med)\n",
        "    for c in X_test_df.columns:\n",
        "        if c in medians:\n",
        "            X_test_df[c] = X_test_df[c].fillna(medians[c])\n",
        "\n",
        "    vt = VarianceThreshold(0.0)\n",
        "    X_train_v = vt.fit_transform(X_train_df)\n",
        "    cols = [c for c,m in zip(X_train_df.columns, vt.get_support()) if m]\n",
        "    X_test_v = X_test_df[cols].values\n",
        "\n",
        "    df_tmp = pd.DataFrame(X_train_v, columns=cols)\n",
        "    skewness = df_tmp.skew()\n",
        "    heavy_cols = skewness[abs(skewness) > skew_threshold].index.tolist()\n",
        "    for c in heavy_cols:\n",
        "        idx = cols.index(c)\n",
        "        X_train_v[:, idx] = np.log1p(np.clip(X_train_v[:, idx],0,None))\n",
        "        X_test_v[:, idx]  = np.log1p(np.clip(X_test_v[:, idx],0,None))\n",
        "\n",
        "    scaler = RobustScaler() if use_robust else StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train_v).astype('float32')\n",
        "    X_test_s  = scaler.transform(X_test_v).astype('float32')\n",
        "    meta = {'cols': cols, 'heavy_cols': heavy_cols, 'scaler': scaler, 'medians': medians}\n",
        "    return X_train_s, X_test_s, meta\n",
        "\n",
        "def transform_apply(X_df, meta):\n",
        "    cols = meta['cols']; heavy_cols = meta['heavy_cols']; scaler = meta['scaler']; medians = meta['medians']\n",
        "    X_df = X_df[cols].copy().apply(pd.to_numeric, errors='coerce')\n",
        "    for c in cols:\n",
        "        if X_df[c].isna().any():\n",
        "            fill = medians.get(c, X_df[c].median())\n",
        "            X_df[c] = X_df[c].fillna(fill)\n",
        "    for c in heavy_cols:\n",
        "        if c in X_df.columns:\n",
        "            X_df[c] = np.log1p(np.clip(X_df[c],0,None))\n",
        "    X_s = scaler.transform(X_df.values).astype('float32')\n",
        "    return X_s"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "utils",
      "metadata": {},
      "source": [
        "## 2. Utilities: class weight, model, training, eval, adaptive (opsional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "utils-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_6234/2075146613.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=USE_AMP)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n",
        "USE_AMP = torch.cuda.is_available()\n",
        "scaler = GradScaler(enabled=USE_AMP)\n",
        "\n",
        "def to_array_features(X):\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        X = X.values\n",
        "    return np.asarray(X, dtype=np.float32)\n",
        "\n",
        "def to_array_labels(y):\n",
        "    if isinstance(y, pd.Series):\n",
        "        y = y.values\n",
        "    y = np.asarray(y)\n",
        "    if y.dtype != np.int64:\n",
        "        y = y.astype(np.int64)\n",
        "    return y\n",
        "\n",
        "def compute_class_weight(y, num_classes):\n",
        "    counts = Counter(y)\n",
        "    total = sum(counts.values())\n",
        "    w = {cls: total / (num_classes * cnt) for cls, cnt in counts.items()}\n",
        "    return torch.tensor([w.get(i, 1.0) for i in range(num_classes)], dtype=torch.float32)\n",
        "\n",
        "def adaptive_balance_mild(X, y, benign_id=None, max_benign_ratio=5.0, rare_min=800, random_state=42):\n",
        "    y = np.asarray(y)\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    counts = Counter(y)\n",
        "    X_new=[]; y_new=[]\n",
        "    # Benign limiting (opsional jika benign_id diberikan)\n",
        "    if benign_id is not None and benign_id in counts and len(counts)>1:\n",
        "        non_benign_max = max(cnt for cls, cnt in counts.items() if cls != benign_id)\n",
        "        target_benign = int(min(counts[benign_id], max_benign_ratio * non_benign_max))\n",
        "        idx = np.where(y==benign_id)[0]\n",
        "        if len(idx) > target_benign:\n",
        "            idx = rng.choice(idx, size=target_benign, replace=False)\n",
        "        X_new.append(X[idx]); y_new.append(y[idx])\n",
        "    # Others\n",
        "    for cls, cnt in counts.items():\n",
        "        if benign_id is not None and cls == benign_id:\n",
        "            continue\n",
        "        idx = np.where(y==cls)[0]\n",
        "        if cnt < rare_min:\n",
        "            extra = rng.choice(idx, size=rare_min - cnt, replace=True)\n",
        "            idx = np.concatenate([idx, extra])\n",
        "        X_new.append(X[idx]); y_new.append(y[idx])\n",
        "    Xb = np.vstack(X_new); yb = np.concatenate(y_new)\n",
        "    perm = rng.permutation(len(yb))\n",
        "    return Xb[perm], yb[perm]\n",
        "\n",
        "def build_dnn(input_dim, layer_sizes, num_classes, activation='relu', dropout=0.35):\n",
        "    acts = {'relu': nn.ReLU(), 'leaky_relu': nn.LeakyReLU(0.01), 'tanh': nn.Tanh()}\n",
        "    seq=[]; prev=input_dim\n",
        "    for h in layer_sizes:\n",
        "        seq += [nn.Linear(prev, h), acts.get(activation, nn.ReLU()), nn.Dropout(dropout)]\n",
        "        prev = h\n",
        "    seq.append(nn.Linear(prev, num_classes))\n",
        "    return nn.Sequential(*seq)\n",
        "\n",
        "def train_dnn_amp(model, X_train, y_train, X_val, y_val, class_weight_tensor,\n",
        "                  lr=0.001, max_epochs=140, batch_size=128, patience=12, accum_steps=1):\n",
        "    X_train = to_array_features(X_train); y_train = to_array_labels(y_train)\n",
        "    X_val   = to_array_features(X_val);   y_val   = to_array_labels(y_val)\n",
        "    ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "    model = model.to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weight_tensor.to(device))\n",
        "    best_loss=float('inf'); best_state=None; wait=0; step=0\n",
        "    X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "    y_val_t = torch.tensor(y_val, dtype=torch.long).to(device)\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train(); opt.zero_grad()\n",
        "        for xb,yb in loader:\n",
        "            xb=xb.to(device); yb=yb.to(device)\n",
        "            if USE_AMP:\n",
        "                with autocast():\n",
        "                    logits = model(xb)\n",
        "                    loss = criterion(logits, yb)\n",
        "                scaler.scale(loss).backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "                step += 1\n",
        "                if step % accum_steps == 0:\n",
        "                    scaler.step(opt); scaler.update(); opt.zero_grad()\n",
        "            else:\n",
        "                logits = model(xb); loss = criterion(logits, yb)\n",
        "                loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "                step += 1\n",
        "                if step % accum_steps == 0:\n",
        "                    opt.step(); opt.zero_grad()\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            if USE_AMP:\n",
        "                with autocast():\n",
        "                    v_logits = model(X_val_t)\n",
        "                    v_loss = criterion(v_logits, y_val_t).item()\n",
        "            else:\n",
        "                v_logits = model(X_val_t); v_loss = criterion(v_logits, y_val_t).item()\n",
        "        if v_loss < best_loss - 1e-4:\n",
        "            best_loss=v_loss; best_state=model.state_dict(); wait=0\n",
        "        else:\n",
        "            wait+=1\n",
        "            if wait>=patience: break\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "def macro_f1_eval(y_true, y_pred):\n",
        "    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    return rep['macro avg']['f1-score'], rep\n",
        "\n",
        "def run_experiment(X_train_df, y_train, X_test_df, y_test,\n",
        "                   layers, activation, dropout, lr, epochs, batch_size,\n",
        "                   use_adaptive=False, benign_factor=5.0, accum_steps=1,\n",
        "                   meta=None, reuse_transform=False):\n",
        "    # Transform fit or reuse\n",
        "    if reuse_transform and meta is not None:\n",
        "        X_train_t = transform_apply(X_train_df[feature_cols], meta)\n",
        "        X_test_t  = transform_apply(X_test_df[feature_cols], meta)\n",
        "        cols = meta['cols']; heavy_cols = meta['heavy_cols']; scaler = meta['scaler']\n",
        "        medians = meta['medians']\n",
        "    else:\n",
        "        X_train_t, X_test_t, meta = transform_fit(X_train_df[feature_cols], X_test_df[feature_cols], skew_threshold=SKEW_THRESHOLD, use_robust=USE_ROBUST_SCALER)\n",
        "        cols = meta['cols']; heavy_cols = meta['heavy_cols']; scaler = meta['scaler']\n",
        "\n",
        "    # Validation split dari training\n",
        "    X_tr_sub, X_val, y_tr_sub, y_val = train_test_split(\n",
        "        X_train_t, to_array_labels(y_train), test_size=0.2, stratify=y_train, random_state=RANDOM_STATE)\n",
        "\n",
        "    # Class weight\n",
        "    class_w = compute_class_weight(y_tr_sub, num_classes)\n",
        "\n",
        "    # Adaptive balancing (opsional)\n",
        "    if use_adaptive:\n",
        "        # benign id\n",
        "        benign_id = label_map.get('BENIGN', None)\n",
        "        X_bal, y_bal = adaptive_balance_mild(X_tr_sub, y_tr_sub, benign_id=benign_id, max_benign_ratio=benign_factor, rare_min=800)\n",
        "    else:\n",
        "        X_bal, y_bal = X_tr_sub, y_tr_sub\n",
        "\n",
        "    # Train\n",
        "    model = build_dnn(X_bal.shape[1], layers, num_classes, activation=activation, dropout=dropout)\n",
        "    model = train_dnn_amp(model, X_bal, y_bal, X_val, y_val, class_w,\n",
        "                          lr=lr, max_epochs=epochs, batch_size=batch_size, patience=12, accum_steps=accum_steps)\n",
        "\n",
        "    # Evaluate test\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.tensor(X_test_t, dtype=torch.float32).to(device))\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "    y_pred = probs.argmax(axis=1)\n",
        "    mf1, rep = macro_f1_eval(to_array_labels(y_test), y_pred)\n",
        "    return mf1, rep, model, meta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phaseA",
      "metadata": {},
      "source": [
        "## 3. Phase A — Batch Eksplorasi Progresif (tanpa adaptive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "phaseA-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch1 best split: 0.8\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>macro_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.8</td>\n",
              "      <td>0.998422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.7</td>\n",
              "      <td>0.998394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.9</td>\n",
              "      <td>0.998087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   split  macro_f1\n",
              "1    0.8  0.998422\n",
              "0    0.7  0.998394\n",
              "2    0.9  0.998087"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>layers</th>\n",
              "      <th>macro_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[512, 256, 128]</td>\n",
              "      <td>0.998522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[256, 256, 128]</td>\n",
              "      <td>0.998189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[256, 256, 256]</td>\n",
              "      <td>0.997989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[256, 128, 128]</td>\n",
              "      <td>0.997803</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            layers  macro_f1\n",
              "3  [512, 256, 128]  0.998522\n",
              "1  [256, 256, 128]  0.998189\n",
              "2  [256, 256, 256]  0.997989\n",
              "0  [256, 128, 128]  0.997803"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>activation</th>\n",
              "      <th>macro_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>leaky_relu</td>\n",
              "      <td>0.998488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>relu</td>\n",
              "      <td>0.995118</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   activation  macro_f1\n",
              "1  leaky_relu  0.998488\n",
              "0        relu  0.995118"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epochs</th>\n",
              "      <th>macro_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60</td>\n",
              "      <td>0.998531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>140</td>\n",
              "      <td>0.998529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>0.998043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   epochs  macro_f1\n",
              "0      60  0.998531\n",
              "2     140  0.998529\n",
              "1     100  0.998043"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lr</th>\n",
              "      <th>macro_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.998343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.003</td>\n",
              "      <td>0.997711</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      lr  macro_f1\n",
              "0  0.001  0.998343\n",
              "1  0.003  0.997711"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>batch_size</th>\n",
              "      <th>macro_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>128</td>\n",
              "      <td>0.998498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>0.998203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32</td>\n",
              "      <td>0.998036</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   batch_size  macro_f1\n",
              "2         128  0.998498\n",
              "1          64  0.998203\n",
              "0          32  0.998036"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phase A Final Config: {'split': 0.8, 'layers': [512, 256, 128], 'activation': 'leaky_relu', 'epoch': 60, 'learning_rate': 0.001, 'batch_size': 128, 'dropout': 0.35}\n"
          ]
        }
      ],
      "source": [
        "phaseA_features = phaseA_df[feature_cols]\n",
        "phaseA_labels   = phaseA_df['Label_encoded']\n",
        "\n",
        "# Batch 1: split\n",
        "split_variants = [0.7, 0.8, 0.9]\n",
        "baseline_layers = [256,128,128]\n",
        "baseline_activation = 'relu'\n",
        "baseline_lr = 0.001\n",
        "baseline_epochs = 40\n",
        "baseline_batch_size = 128\n",
        "\n",
        "batch1=[]\n",
        "for sr in split_variants:\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(phaseA_features, phaseA_labels, train_size=sr, stratify=phaseA_labels, random_state=RANDOM_STATE)\n",
        "    mf1, rep, model, meta = run_experiment(X_tr, y_tr, X_te, y_te,\n",
        "        layers=baseline_layers, activation=baseline_activation, dropout=0.35,\n",
        "        lr=baseline_lr, epochs=baseline_epochs, batch_size=baseline_batch_size,\n",
        "        use_adaptive=False, benign_factor=5.0, accum_steps=1,\n",
        "        meta=None, reuse_transform=False)\n",
        "    batch1.append({'split': sr, 'macro_f1': mf1})\n",
        "batch1_df = pd.DataFrame(batch1).sort_values('macro_f1', ascending=False)\n",
        "best_split = float(batch1_df.iloc[0]['split'])\n",
        "print(\"Batch1 best split:\", best_split)\n",
        "display(batch1_df)\n",
        "\n",
        "# Fix split untuk batch berikut\n",
        "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(\n",
        "    phaseA_features, phaseA_labels, train_size=best_split, stratify=phaseA_labels, random_state=RANDOM_STATE)\n",
        "\n",
        "# Fit transform sekali & reuse\n",
        "_, _, freeze_meta = transform_fit(X_train_all[feature_cols], X_test_all[feature_cols], skew_threshold=SKEW_THRESHOLD, use_robust=USE_ROBUST_SCALER)\n",
        "\n",
        "# Batch 2: hidden layers\n",
        "layer_variants = [[256,128,128],[256,256,128],[256,256,256],[512,256,128]]\n",
        "batch2=[]\n",
        "for layers in layer_variants:\n",
        "    mf1, rep, model, _ = run_experiment(X_train_all, y_train_all, X_test_all, y_test_all,\n",
        "        layers=layers, activation='relu', dropout=0.35, lr=baseline_lr,\n",
        "        epochs=50, batch_size=baseline_batch_size,\n",
        "        use_adaptive=False, benign_factor=5.0, accum_steps=1,\n",
        "        meta=freeze_meta, reuse_transform=True)\n",
        "    batch2.append({'layers': str(layers), 'macro_f1': mf1})\n",
        "batch2_df = pd.DataFrame(batch2).sort_values('macro_f1', ascending=False)\n",
        "top_layers = eval(batch2_df.iloc[0]['layers'])\n",
        "display(batch2_df)\n",
        "\n",
        "# Batch 3: activation (tetap simple, relu & leaky_relu)\n",
        "activation_variants = ['relu','leaky_relu']\n",
        "batch3=[]\n",
        "for act in activation_variants:\n",
        "    mf1, rep, model, _ = run_experiment(X_train_all, y_train_all, X_test_all, y_test_all,\n",
        "        layers=top_layers, activation=act, dropout=0.35, lr=baseline_lr,\n",
        "        epochs=50, batch_size=baseline_batch_size,\n",
        "        use_adaptive=False, benign_factor=5.0, accum_steps=1,\n",
        "        meta=freeze_meta, reuse_transform=True)\n",
        "    batch3.append({'activation': act, 'macro_f1': mf1})\n",
        "batch3_df = pd.DataFrame(batch3).sort_values('macro_f1', ascending=False)\n",
        "top_activation = batch3_df.iloc[0]['activation']\n",
        "display(batch3_df)\n",
        "\n",
        "# Batch 4: epochs (warm/long)\n",
        "epoch_variants = [60, 100, 140]\n",
        "batch4=[]\n",
        "for ep in epoch_variants:\n",
        "    mf1, rep, model, _ = run_experiment(X_train_all, y_train_all, X_test_all, y_test_all,\n",
        "        layers=top_layers, activation=top_activation, dropout=0.35, lr=baseline_lr,\n",
        "        epochs=ep, batch_size=baseline_batch_size,\n",
        "        use_adaptive=False, benign_factor=5.0, accum_steps=1,\n",
        "        meta=freeze_meta, reuse_transform=True)\n",
        "    batch4.append({'epochs': ep, 'macro_f1': mf1})\n",
        "batch4_df = pd.DataFrame(batch4).sort_values('macro_f1', ascending=False)\n",
        "top_epochs = int(batch4_df.iloc[0]['epochs'])\n",
        "display(batch4_df)\n",
        "\n",
        "# Batch 5: learning rate\n",
        "lr_variants = [0.001, 0.003]\n",
        "batch5=[]\n",
        "for lr in lr_variants:\n",
        "    mf1, rep, model, _ = run_experiment(X_train_all, y_train_all, X_test_all, y_test_all,\n",
        "        layers=top_layers, activation=top_activation, dropout=0.35, lr=lr,\n",
        "        epochs=top_epochs, batch_size=baseline_batch_size,\n",
        "        use_adaptive=False, benign_factor=5.0, accum_steps=1,\n",
        "        meta=freeze_meta, reuse_transform=True)\n",
        "    batch5.append({'lr': lr, 'macro_f1': mf1})\n",
        "batch5_df = pd.DataFrame(batch5).sort_values('macro_f1', ascending=False)\n",
        "top_lr = float(batch5_df.iloc[0]['lr'])\n",
        "display(batch5_df)\n",
        "\n",
        "# Batch 6: batch size\n",
        "batch_size_variants = [32, 64, 128]\n",
        "batch6=[]\n",
        "for bs in batch_size_variants:\n",
        "    mf1, rep, model, _ = run_experiment(X_train_all, y_train_all, X_test_all, y_test_all,\n",
        "        layers=top_layers, activation=top_activation, dropout=0.35, lr=top_lr,\n",
        "        epochs=top_epochs, batch_size=bs,\n",
        "        use_adaptive=False, benign_factor=5.0, accum_steps=1 if bs<=128 else 2,\n",
        "        meta=freeze_meta, reuse_transform=True)\n",
        "    batch6.append({'batch_size': bs, 'macro_f1': mf1})\n",
        "batch6_df = pd.DataFrame(batch6).sort_values('macro_f1', ascending=False)\n",
        "top_bs = int(batch6_df.iloc[0]['batch_size'])\n",
        "display(batch6_df)\n",
        "\n",
        "phaseA_final_config = {\n",
        "    'split': best_split,\n",
        "    'layers': top_layers,\n",
        "    'activation': top_activation,\n",
        "    'epoch': top_epochs,\n",
        "    'learning_rate': top_lr,\n",
        "    'batch_size': top_bs,\n",
        "    'dropout': 0.35\n",
        "}\n",
        "print(\"Phase A Final Config:\", phaseA_final_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phaseB",
      "metadata": {},
      "source": [
        "## 4. Phase B — Final Training (default tanpa adaptive; opsi banding adaptive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "phaseB-code",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>use_adaptive</th>\n",
              "      <th>macro_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>0.986720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>True</td>\n",
              "      <td>0.985643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   use_adaptive  macro_f1\n",
              "0         False  0.986720\n",
              "1          True  0.985643"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Phase B config → use_adaptive: False Macro F1: 0.9867195533221962\n",
            "Final Config: {'split_ratio': 0.8, 'layers': [512, 256, 128], 'activation': 'leaky_relu', 'learning_rate': 0.001, 'epochs': 60, 'batch_size': 128, 'dropout': 0.35, 'use_adaptive': False, 'adaptive_params': {'benign_factor': 5.0, 'rare_min': 800}}\n",
            "Final Macro F1: 0.9867195533221962\n"
          ]
        }
      ],
      "source": [
        "final_split = float(phaseA_final_config['split'])\n",
        "X_full = full_df[feature_cols]\n",
        "y_full = full_df['Label_encoded']\n",
        "\n",
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
        "    X_full, y_full, train_size=final_split, stratify=y_full, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Lock transform sekali untuk Phase B\n",
        "_, _, phaseB_meta = transform_fit(X_train_full[feature_cols], X_test_full[feature_cols], skew_threshold=SKEW_THRESHOLD, use_robust=USE_ROBUST_SCALER)\n",
        "\n",
        "# Config dasar dari Phase A\n",
        "base_layers = phaseA_final_config['layers']\n",
        "base_activation = phaseA_final_config['activation']\n",
        "base_epoch = int(phaseA_final_config['epoch'])\n",
        "base_lr = float(phaseA_final_config['learning_rate'])\n",
        "base_batch = int(phaseA_final_config['batch_size'])\n",
        "\n",
        "# Evaluasi dua opsi: non-adaptive vs adaptive mild (pilih terbaik)\n",
        "results_phaseB = []\n",
        "for use_adapt in [False, True]:\n",
        "    mf1, rep, model, _ = run_experiment(\n",
        "        X_train_full, y_train_full, X_test_full, y_test_full,\n",
        "        layers=base_layers, activation=base_activation, dropout=0.35, lr=base_lr,\n",
        "        epochs=base_epoch, batch_size=base_batch,\n",
        "        use_adaptive=use_adapt, benign_factor=5.0, accum_steps=1 if base_batch<=128 else 2,\n",
        "        meta=phaseB_meta, reuse_transform=True)\n",
        "    results_phaseB.append({'use_adaptive': use_adapt, 'macro_f1': mf1, 'rep': rep, 'model': model})\n",
        "\n",
        "phaseB_df = pd.DataFrame([{k:v for k,v in r.items() if k!='rep' and k!='model'} for r in results_phaseB]).sort_values('macro_f1', ascending=False)\n",
        "display(phaseB_df)\n",
        "best_phaseB_row = phaseB_df.iloc[0].to_dict()\n",
        "best_use_adapt = bool(best_phaseB_row['use_adaptive'])\n",
        "best_model_idx = phaseB_df.index[0]\n",
        "best_model = results_phaseB[best_model_idx]['model']\n",
        "best_rep = results_phaseB[best_model_idx]['rep']\n",
        "print(\"Best Phase B config → use_adaptive:\", best_use_adapt, \"Macro F1:\", best_phaseB_row['macro_f1'])\n",
        "\n",
        "final_config = {\n",
        "    'split_ratio': final_split,\n",
        "    'layers': base_layers,\n",
        "    'activation': base_activation,\n",
        "    'learning_rate': base_lr,\n",
        "    'epochs': base_epoch,\n",
        "    'batch_size': base_batch,\n",
        "    'dropout': 0.35,\n",
        "    'use_adaptive': best_use_adapt,\n",
        "    'adaptive_params': {'benign_factor': 5.0, 'rare_min': 800}\n",
        "}\n",
        "print(\"Final Config:\", final_config)\n",
        "final_model = best_model\n",
        "final_rep = best_rep\n",
        "mf1_final = final_rep['macro avg']['f1-score']\n",
        "print(\"Final Macro F1:\", mf1_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d84acb",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# ==== Results & Visualization (rapi) ====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
        "\n",
        "def _class_name(idx, inv_label_map):\n",
        "    try:\n",
        "        return inv_label_map[int(idx)]\n",
        "    except Exception:\n",
        "        return str(idx)\n",
        "\n",
        "def _report_to_df(rep_dict, inv_label_map):\n",
        "    rows = []\n",
        "    for k, v in rep_dict.items():\n",
        "        if k in (\"accuracy\", \"macro avg\", \"weighted avg\"):\n",
        "            continue\n",
        "        # k bisa string dari angka (\"0\",\"1\",...) atau langsung nama kelas\n",
        "        try:\n",
        "            cls_idx = int(k)\n",
        "            cls_name = inv_label_map.get(cls_idx, str(k))\n",
        "        except:\n",
        "            cls_idx = None\n",
        "            cls_name = k\n",
        "        rows.append({\n",
        "            \"class_id\": cls_idx,\n",
        "            \"class_name\": cls_name,\n",
        "            \"precision\": v.get(\"precision\", np.nan),\n",
        "            \"recall\": v.get(\"recall\", np.nan),\n",
        "            \"f1\": v.get(\"f1-score\", np.nan),\n",
        "            \"support\": v.get(\"support\", np.nan)\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    if \"class_id\" in df.columns:\n",
        "        df = df.sort_values([\"f1\",\"class_name\"], ascending=[True, True], na_position=\"last\")\n",
        "    return df\n",
        "\n",
        "def _transform_apply(df_raw, meta, use_cols):\n",
        "    # Terapkan transform locked yang sama dengan phaseB\n",
        "    cols = meta[\"cols\"]; heavy_cols = meta[\"heavy_cols\"]; scaler = meta[\"scaler\"]; medians = meta.get(\"medians\", {})\n",
        "    X = df_raw[cols].copy().apply(pd.to_numeric, errors=\"coerce\")\n",
        "    for c in cols:\n",
        "        if X[c].isna().any():\n",
        "            X[c] = X[c].fillna(medians.get(c, X[c].median()))\n",
        "    for c in heavy_cols:\n",
        "        if c in X.columns:\n",
        "            X[c] = np.log1p(np.clip(X[c], 0, None))\n",
        "    return scaler.transform(X.values).astype(\"float32\")\n",
        "\n",
        "# 1) Siapkan X_test, y_test\n",
        "X_test_arr = _transform_apply(X_test_full[feature_cols], phaseB_meta, feature_cols)\n",
        "y_true = y_test_full.to_numpy() if hasattr(y_test_full, \"to_numpy\") else np.asarray(y_test_full)\n",
        "\n",
        "# 2) Prediksi\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = final_model(torch.tensor(X_test_arr, dtype=torch.float32).to(device))\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "y_pred = probs.argmax(axis=1)\n",
        "\n",
        "# 3) Metrik ringkas\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "weighted_f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "print(f\"Accuracy:    {acc:.4f}\")\n",
        "print(f\"Macro F1:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
        "\n",
        "# 4) Classification report (tabel rapi)\n",
        "rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "df_cls = _report_to_df(rep, inv_label_map)\n",
        "display(df_cls)\n",
        "\n",
        "# 5) Bar chart F1 per kelas (urut dari terendah)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=df_cls.sort_values(\"f1\"), x=\"f1\", y=\"class_name\", palette=\"viridis\")\n",
        "plt.title(\"F1 per Kelas (Ascending)\")\n",
        "plt.xlabel(\"F1-score\")\n",
        "plt.ylabel(\"Class\")\n",
        "plt.xlim(0, 1.0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6) Confusion Matrix (normalisasi baris)\n",
        "labels_idx = sorted(np.unique(np.concatenate([y_true, y_pred])))\n",
        "labels_name = [_class_name(i, inv_label_map) for i in labels_idx]\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels_idx)\n",
        "cm_row = cm.astype(\"float\") / (cm.sum(axis=1, keepdims=True) + 1e-9)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_row, annot=False, cmap=\"Blues\", xticklabels=labels_name, yticklabels=labels_name, cbar=True, vmin=0, vmax=1)\n",
        "plt.title(\"Confusion Matrix (Row-normalized)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 7) Top-10 pasangan kelas paling sering salah (off-diagonal)\n",
        "mis_rows = []\n",
        "for i, ti in enumerate(labels_idx):\n",
        "    for j, pj in enumerate(labels_idx):\n",
        "        if i == j: \n",
        "            continue\n",
        "        cnt = cm[i, j]\n",
        "        if cnt > 0:\n",
        "            mis_rows.append({\n",
        "                \"true_id\": ti, \"true\": _class_name(ti, inv_label_map),\n",
        "                \"pred_id\": pj, \"pred\": _class_name(pj, inv_label_map),\n",
        "                \"count\": int(cnt),\n",
        "                \"pct_of_true\": float(cnt) / (cm[i].sum() + 1e-9)\n",
        "            })\n",
        "df_mis = pd.DataFrame(mis_rows).sort_values([\"count\",\"pct_of_true\"], ascending=[False, False]).head(10)\n",
        "print(\"Top-10 Misclassifications (by count):\")\n",
        "display(df_mis)\n",
        "\n",
        "# 8) Distribusi kelas di test (ground truth vs prediksi)\n",
        "dist_true = pd.Series(y_true).value_counts().reindex(labels_idx, fill_value=0).rename(index=lambda i: _class_name(i, inv_label_map))\n",
        "dist_pred = pd.Series(y_pred).value_counts().reindex(labels_idx, fill_value=0).rename(index=lambda i: _class_name(i, inv_label_map))\n",
        "df_dist = pd.DataFrame({\"true_count\": dist_true, \"pred_count\": dist_pred})\n",
        "display(df_dist)\n",
        "\n",
        "# 9) Simpan hasil (opsional)\n",
        "try:\n",
        "    # Tabel per kelas\n",
        "    df_cls.to_csv(ART_DIR / \"per_class_metrics.csv\", index=False)\n",
        "    # Misclass top10\n",
        "    df_mis.to_csv(ART_DIR / \"top10_misclass.csv\", index=False)\n",
        "    # Distribusi\n",
        "    df_dist.to_csv(ART_DIR / \"test_distribution.csv\")\n",
        "    # Confusion matrix (CSV)\n",
        "    cm_df = pd.DataFrame(cm, index=labels_name, columns=labels_name)\n",
        "    cm_df.to_csv(ART_DIR / \"confusion_matrix_raw.csv\")\n",
        "    cm_row_df = pd.DataFrame(cm_row, index=labels_name, columns=labels_name)\n",
        "    cm_row_df.to_csv(ART_DIR / \"confusion_matrix_row_normalized.csv\")\n",
        "\n",
        "    # Simpan plot PNG\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.barplot(data=df_cls.sort_values(\"f1\"), x=\"f1\", y=\"class_name\", palette=\"viridis\")\n",
        "    plt.title(\"F1 per Kelas (Ascending)\")\n",
        "    plt.xlabel(\"F1-score\"); plt.ylabel(\"Class\"); plt.xlim(0,1); plt.tight_layout()\n",
        "    plt.savefig(ART_DIR / \"bar_f1_per_class.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm_row, annot=False, cmap=\"Blues\", xticklabels=labels_name, yticklabels=labels_name, cbar=True, vmin=0, vmax=1)\n",
        "    plt.title(\"Confusion Matrix (Row-normalized)\")\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.tight_layout()\n",
        "    plt.savefig(ART_DIR / \"confusion_matrix_row_norm.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    with open(ART_DIR / \"summary_metrics.txt\",\"w\") as f:\n",
        "        f.write(f\"Accuracy: {acc:.6f}\\nMacro F1: {macro_f1:.6f}\\nWeighted F1: {weighted_f1:.6f}\\n\")\n",
        "\n",
        "    print(\"Hasil disimpan ke:\", ART_DIR)\n",
        "except Exception as e:\n",
        "    print(\"Gagal menyimpan hasil ke ART_DIR:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4603fe4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluasi di TRAIN FULL\n",
        "X_train_eval = transform_apply(X_train_full[feature_cols], phaseB_meta)\n",
        "y_train_eval = y_train_full.to_numpy() if hasattr(y_train_full, \"to_numpy\") else np.asarray(y_train_full)\n",
        "\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    train_logits = final_model(torch.tensor(X_train_eval, dtype=torch.float32).to(device))\n",
        "    train_probs = torch.softmax(train_logits, dim=1).cpu().numpy()\n",
        "train_pred = train_probs.argmax(axis=1)\n",
        "\n",
        "# Evaluasi di TEST FULL (kamu sudah punya X_test_arr atau bisa ulang)\n",
        "X_test_eval = transform_apply(X_test_full[feature_cols], phaseB_meta)\n",
        "y_test_eval = y_test_full.to_numpy() if hasattr(y_test_full, \"to_numpy\") else np.asarray(y_test_full)\n",
        "with torch.no_grad():\n",
        "    test_logits = final_model(torch.tensor(X_test_eval, dtype=torch.float32).to(device))\n",
        "    test_probs = torch.softmax(test_logits, dim=1).cpu().numpy()\n",
        "test_pred = test_probs.argmax(axis=1)\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "train_macro_f1 = f1_score(y_train_eval, train_pred, average='macro', zero_division=0)\n",
        "test_macro_f1 = f1_score(y_test_eval, test_pred, average='macro', zero_division=0)\n",
        "\n",
        "print(f\"Train Macro F1: {train_macro_f1:.4f}\")\n",
        "print(f\"Test  Macro F1: {test_macro_f1:.4f}\")\n",
        "print(f\"Generalization gap (Train - Test): {train_macro_f1 - test_macro_f1:.4f}\")\n",
        "\n",
        "# (Opsional) Per-class coba:\n",
        "rep_train = classification_report(y_train_eval, train_pred, output_dict=True, zero_division=0)\n",
        "rep_test  = classification_report(y_test_eval, test_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "def per_class_f1(rep):\n",
        "    out=[]\n",
        "    for k,v in rep.items():\n",
        "        if k in ['accuracy','macro avg','weighted avg']: continue\n",
        "        try:\n",
        "            cls_id = int(k)\n",
        "        except:\n",
        "            continue\n",
        "        out.append((cls_id, v['f1-score']))\n",
        "    return dict(out)\n",
        "\n",
        "train_f1_per = per_class_f1(rep_train)\n",
        "test_f1_per  = per_class_f1(rep_test)\n",
        "\n",
        "print(\"Per-class F1 gap (train - test):\")\n",
        "for cls_id in sorted(train_f1_per.keys()):\n",
        "    gap = train_f1_per[cls_id] - test_f1_per.get(cls_id, 0)\n",
        "    print(f\"Class {cls_id}: Train {train_f1_per[cls_id]:.3f} | Test {test_f1_per.get(cls_id,0):.3f} | Gap {gap:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inference",
      "metadata": {},
      "source": [
        "## 5. Inference Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inference-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['BENIGN', 'BENIGN', 'BENIGN', 'BENIGN', 'BENIGN']\n"
          ]
        }
      ],
      "source": [
        "cols_final = phaseB_meta['cols']\n",
        "heavy_final = phaseB_meta['heavy_cols']\n",
        "scaler_final = phaseB_meta['scaler']\n",
        "medians_final = phaseB_meta['medians']\n",
        "\n",
        "def predict_label_names(df_raw_subset):\n",
        "    # Terapkan transform yang sama\n",
        "    X_arr = transform_apply(df_raw_subset[feature_cols], phaseB_meta)\n",
        "    with torch.no_grad():\n",
        "        logits = final_model(torch.tensor(X_arr, dtype=torch.float32).to(device))\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "    y_pred = probs.argmax(axis=1)\n",
        "    labels = [inv_label_map[i] for i in y_pred]\n",
        "    return labels\n",
        "\n",
        "# Contoh penggunaan\n",
        "print(predict_label_names(X_test_full.head(5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save",
      "metadata": {},
      "source": [
        "## 6. Simpan Artefak Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save-code",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'final_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mfinal_model\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), ART_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_model_state.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(scaler_final, ART_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_scaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ART_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_transform_meta.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_model' is not defined"
          ]
        }
      ],
      "source": [
        "torch.save(final_model.state_dict(), ART_DIR / 'final_model_state.pt')\n",
        "joblib.dump(scaler_final, ART_DIR / 'final_scaler.pkl')\n",
        "with open(ART_DIR / 'final_transform_meta.json','w') as f:\n",
        "    json.dump({'cols_final': cols_final, 'heavy_final': heavy_final, 'medians': medians_final}, f, indent=2)\n",
        "with open(ART_DIR / 'final_config.json', 'w') as f:\n",
        "    json.dump(final_config, f, indent=2)\n",
        "with open(ART_DIR / 'final_report.json', 'w') as f:\n",
        "    json.dump({'macro_f1': mf1_final, 'classification_report': final_rep, 'label_map': label_map}, f, indent=2)\n",
        "print(\"Artefak final disimpan ke:\", ART_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0d4edf",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alfanahmuhson/inference_new_dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_file, map_location='cpu')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Saran mapping kolom (harap cek manual jika perlu) ==\n",
            "               expected_col          expected_canonical                                  suggested_candidates\n",
            "           Destination Port            destination_port                                            [Dst Port]\n",
            "              Flow Duration               flow_duration          [Flow Duration, Flow IAT Min, Flow IAT Mean]\n",
            "          Total Fwd Packets           total_fwd_packets         [Tot Fwd Pkts, TotLen Fwd Pkts, Tot Bwd Pkts]\n",
            "Total Length of Fwd Packets total_length_of_fwd_packets      [TotLen Fwd Pkts, TotLen Bwd Pkts, Tot Fwd Pkts]\n",
            "      Fwd Packet Length Max       fwd_packet_length_max  [Fwd Pkt Len Max, Bwd Pkt Len Max, Fwd Pkt Len Mean]\n",
            "      Fwd Packet Length Min       fwd_packet_length_min  [Fwd Pkt Len Min, Bwd Pkt Len Min, Fwd Pkt Len Mean]\n",
            "     Fwd Packet Length Mean      fwd_packet_length_mean [Fwd Pkt Len Mean, Bwd Pkt Len Mean, Fwd Pkt Len Min]\n",
            "      Fwd Packet Length Std       fwd_packet_length_std       [Fwd Pkt Len Std, Bwd Pkt Len Std, Pkt Len Std]\n",
            "      Bwd Packet Length Max       bwd_packet_length_max  [Bwd Pkt Len Max, Fwd Pkt Len Max, Bwd Pkt Len Mean]\n",
            "      Bwd Packet Length Min       bwd_packet_length_min  [Bwd Pkt Len Min, Fwd Pkt Len Min, Bwd Pkt Len Mean]\n",
            "     Bwd Packet Length Mean      bwd_packet_length_mean [Bwd Pkt Len Mean, Fwd Pkt Len Mean, Bwd Pkt Len Min]\n",
            "               Flow Bytes/s                flow_bytes/s          [Flow Byts/s, Flow Pkts/s, Subflow Fwd Byts]\n",
            "             Flow Packets/s              flow_packets/s                [Flow Pkts/s, Fwd Pkts/s, Flow Byts/s]\n",
            "              Flow IAT Mean               flow_iat_mean           [Flow IAT Mean, Fwd IAT Mean, Flow IAT Min]\n",
            "               Flow IAT Std                flow_iat_std              [Flow IAT Std, Fwd IAT Std, Bwd IAT Std]\n",
            "               Flow IAT Max                flow_iat_max            [Flow IAT Max, Flow IAT Mean, Fwd IAT Max]\n",
            "               Flow IAT Min                flow_iat_min            [Flow IAT Min, Flow IAT Mean, Fwd IAT Min]\n",
            "               Fwd IAT Mean                fwd_iat_mean           [Fwd IAT Mean, Bwd IAT Mean, Flow IAT Mean]\n",
            "                Fwd IAT Std                 fwd_iat_std              [Fwd IAT Std, Bwd IAT Std, Flow IAT Std]\n",
            "                Fwd IAT Min                 fwd_iat_min              [Fwd IAT Min, Bwd IAT Min, Fwd IAT Mean]\n",
            "              Bwd IAT Total               bwd_iat_total               [Bwd IAT Tot, Fwd IAT Tot, Bwd IAT Std]\n",
            "               Bwd IAT Mean                bwd_iat_mean             [Bwd IAT Mean, Fwd IAT Mean, Bwd IAT Min]\n",
            "                Bwd IAT Std                 bwd_iat_std               [Bwd IAT Std, Fwd IAT Std, Bwd IAT Tot]\n",
            "                Bwd IAT Max                 bwd_iat_max              [Bwd IAT Max, Fwd IAT Max, Bwd IAT Mean]\n",
            "                Bwd IAT Min                 bwd_iat_min              [Bwd IAT Min, Fwd IAT Min, Bwd IAT Mean]\n",
            "              Fwd PSH Flags               fwd_psh_flags         [Fwd PSH Flags, Bwd PSH Flags, Fwd URG Flags]\n",
            "              Fwd URG Flags               fwd_urg_flags         [Fwd URG Flags, Bwd URG Flags, Fwd PSH Flags]\n",
            "          Fwd Header Length           fwd_header_length                      [Fwd Header Len, Bwd Header Len]\n",
            "          Bwd Header Length           bwd_header_length                      [Bwd Header Len, Fwd Header Len]\n",
            "              Bwd Packets/s               bwd_packets/s                 [Bwd Pkts/s, Fwd Pkts/s, Flow Pkts/s]\n",
            "          Min Packet Length           min_packet_length                                                    []\n",
            "          Max Packet Length           max_packet_length                                                    []\n",
            "         Packet Length Mean          packet_length_mean    [Pkt Len Mean, Fwd Pkt Len Mean, Bwd Pkt Len Mean]\n",
            "     Packet Length Variance      packet_length_variance                            [Pkt Len Var, Pkt Len Min]\n",
            "             FIN Flag Count              fin_flag_count          [FIN Flag Cnt, CWE Flag Count, SYN Flag Cnt]\n",
            "             RST Flag Count              rst_flag_count          [RST Flag Cnt, CWE Flag Count, URG Flag Cnt]\n",
            "             PSH Flag Count              psh_flag_count          [PSH Flag Cnt, CWE Flag Count, SYN Flag Cnt]\n",
            "             ACK Flag Count              ack_flag_count          [ACK Flag Cnt, CWE Flag Count, ECE Flag Cnt]\n",
            "             URG Flag Count              urg_flag_count          [URG Flag Cnt, CWE Flag Count, RST Flag Cnt]\n",
            "              Down/Up Ratio               down/up_ratio                        [Down/Up Ratio, Flow Duration]\n",
            "     Init_Win_bytes_forward      init_win_bytes_forward                [Init Fwd Win Byts, Init Bwd Win Byts]\n",
            "    Init_Win_bytes_backward     init_win_bytes_backward                [Init Fwd Win Byts, Init Bwd Win Byts]\n",
            "           act_data_pkt_fwd            act_data_pkt_fwd                                   [Fwd Act Data Pkts]\n",
            "       min_seg_size_forward        min_seg_size_forward                  [Fwd Seg Size Avg, Bwd Seg Size Avg]\n",
            "                Active Mean                 active_mean                 [Active Mean, Active Min, Active Max]\n",
            "                 Active Std                  active_std                  [Active Std, Active Min, Active Max]\n",
            "                 Active Max                  active_max                 [Active Max, Active Mean, Active Min]\n",
            "                 Active Min                  active_min                 [Active Min, Active Mean, Active Max]\n",
            "                  Idle Mean                   idle_mean                       [Idle Mean, Idle Min, Idle Max]\n",
            "                   Idle Std                    idle_std                   [Idle Std, Active Std, Pkt Len Std]\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'flow_duration_ms'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_gpu/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'flow_duration_ms'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Buat manual map jika ada nama beda\u001b[39;00m\n\u001b[1;32m     10\u001b[0m manual_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlow Duration\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflow_duration_ms\u001b[39m\u001b[38;5;124m'\u001b[39m,   \u001b[38;5;66;03m# contoh\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Fwd Packets\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward_packets_total\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# tambahkan sesuai kebutuhan\u001b[39;00m\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanual_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediksi label pertama:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m20\u001b[39m])\n",
            "File \u001b[0;32m~/inference_new_dataset.py:271\u001b[0m, in \u001b[0;36mpredict_batch\u001b[0;34m(new_df, artifacts, manual_map, auto_strict, fill_missing_with, return_proba)\u001b[0m\n\u001b[1;32m    269\u001b[0m model \u001b[38;5;241m=\u001b[39m artifacts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    270\u001b[0m inv_label_map \u001b[38;5;241m=\u001b[39m artifacts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minv_label_map\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 271\u001b[0m X_arr \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmanual_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanual_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mauto_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_strict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfill_missing_with\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_missing_with\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    277\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mtensor(X_arr, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n",
            "File \u001b[0;32m~/inference_new_dataset.py:234\u001b[0m, in \u001b[0;36mprepare_features\u001b[0;34m(new_df, artifacts, manual_map, auto_strict, fill_missing_with)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill_missing_with harus \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m atau \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzero\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     ser \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(\u001b[43mnew_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mactual_col\u001b[49m\u001b[43m]\u001b[49m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Imputasi nilai NaN di kolom actual memakai median training atau median kolom baru\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     fill_med \u001b[38;5;241m=\u001b[39m medians\u001b[38;5;241m.\u001b[39mget(exp_col, ser\u001b[38;5;241m.\u001b[39mmedian())\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_gpu/lib/python3.9/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_gpu/lib/python3.9/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'flow_duration_ms'"
          ]
        }
      ],
      "source": [
        "from inference_new_dataset import load_artifacts, audit_new_dataset_columns, predict_batch\n",
        "\n",
        "artifacts = load_artifacts(\"./artifacts_multiclass_collapsed_v4\")\n",
        "new_df = pd.read_csv(\"Brute Force.csv\")\n",
        "\n",
        "# Lihat saran mapping\n",
        "audit_new_dataset_columns(new_df, artifacts)\n",
        "\n",
        "# Buat manual map jika ada nama beda\n",
        "manual_map = {\n",
        "    'Flow Duration': 'flow_duration_ms',   # contoh\n",
        "    'Total Fwd Packets': 'forward_packets_total',\n",
        "    # tambahkan sesuai kebutuhan\n",
        "}\n",
        "\n",
        "result = predict_batch(new_df, artifacts, manual_map=manual_map, auto_strict=False)\n",
        "print(\"Prediksi label pertama:\", result['predictions'][:20])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "jupyter_gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.24"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
